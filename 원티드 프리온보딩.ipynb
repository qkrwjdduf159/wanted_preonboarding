{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"원티드 프리온보딩.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOAWaV2gl3DPr+0RUf2C9Dd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 1번 문제"],"metadata":{"id":"8f6LSL9IN0BO"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"g0R7zBt2oeIM","executionInfo":{"status":"ok","timestamp":1644679432901,"user_tz":-540,"elapsed":13,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"outputs":[],"source":["import re\n","\n","class Tokenizer():\n","    def __init__(self):\n","        self.word_dict = {'oov':0}\n","        self.fit_checker = False\n","\n","    def preprocessing(self, sequences):\n","        \n","        result = []\n","        \n","        # 조건 1: 입력된 문장에 대해서 소문자로의 변환과 특수문자 제거를 수행합니다.\n","        for string in sequences:\n","            string = re.sub(r\"[^a-zA-Z0-9 ]\",'',string)\n","            string = string.lower()\n","            # 조건 2: 토큰화는 white space 단위로 수행합니다.\n","            list_spring = string.split()\n","\n","            result.append(list_spring)\n","\n","        return result\n","\n","    def fit(self, sequences):\n","        self.fit_checker = False\n","        \n","        # 조건 1: 위에서 만든 preprocessing 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n","        token = self.preprocessing(sequences)\n","        ## num_token을 통해서 값을 하나하나 지정해 주어야 한다.##\n","        num_token = 1\n","        # 조건 2: 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(self.word_dict)을 생성합니다.\n","        for string in token:\n","            for value in string:\n","\n","                if value not in self.word_dict.keys():\n","                    self.word_dict[value] = num_token\n","                    num_token += 1\n","                else:\n","                    pass\n","\n","        self.fit_checker = True\n","\n","    def transform(self, sequences):\n","\n","        result = []\n","\n","        tokens = self.preprocessing(sequences)\n","        if self.fit_checker:\n","            for string in tokens:\n","                string_result = []\n","                for token in range(len(string)):\n","                    try:\n","                        value = self.word_dict[string[token]]\n","                        string_result.append(value)\n","                    except:\n","                        value = self.word_dict['oov']\n","                        string_result.append(value)\n","                result.append(string_result)\n","            return result\n","        else:\n","            raise Exception('Tokenizer instance is not fitted yet.')\n","\n","    def fit_transform(self, sequences):\n","        self.fit(sequences)\n","        result = self.transform(sequences)\n","        return result"]},{"cell_type":"code","source":["list_sequence = ['I have computer', 'I hate soccer', 'I Love Coffee', 'Good Weather']\n","list_sequence1 = ['I have keyboard', 'I hate excercise', 'hahahahohoho', 'I study AI']\n","Token = Tokenizer()\n","Token.fit(list_sequence)\n","print(Token.transform(list_sequence1))\n","print(Token.fit_transform(list_sequence))"],"metadata":{"id":"6DsUFRZj-M7F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644679432902,"user_tz":-540,"elapsed":12,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"d89df35e-15da-42b6-8ef7-14745549868a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 0], [1, 4, 0], [0], [1, 0, 0]]\n","[[1, 2, 3], [1, 4, 5], [1, 6, 7], [8, 9]]\n"]}]},{"cell_type":"markdown","source":["## 2번 문제"],"metadata":{"id":"BqsDQE-AN2XJ"}},{"cell_type":"code","source":["## TfidVectorizer 생성하기\n","import numpy as np\n","\n","class TfidfVectorizer:\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","        self.fit_checker = False\n","\n","    def fit(self, sequences):\n","        tokenized = self.tokenizer.fit_transform(sequences)\n","        '''\n","        문제 2-1\n","        '''\n","        num_string = len(tokenized)\n","        self.fit_token_unique = len(np.unique([j for i in tokenized for j in i]))\n","        self.fit_token_min = np.min([j for i in tokenized for j in i])\n","\n","        results = []\n","        for i in tokenized:\n","            string = [0 for i in range(self.fit_token_unique)]\n","            for j in i:\n","                string[j - self.fit_token_min] += 1\n","\n","            results.append(string)\n","\n","        matrix_result = np.matrix(results).sum(axis = 0)\n","        self.IDF = np.log(num_string/(1 + matrix_result)).tolist()[0]\n","        self.fit_checker = True\n","\n","    def transform(self, sequences):\n","        if self.fit_checker:\n","            tokenized = self.tokenizer.transform(sequences)\n","\n","            tf_results = []\n","            for i in tokenized:\n","                string = [0 for i in range(self.fit_token_unique)]\n","                for j in i:\n","                    string[j - self.fit_token_min] += 1.0\n","\n","                tf_results.append(string)\n","\n","            self.tfidf_matrix = []\n","            for string in tf_results:\n","                tfidf_list = [i*j for i,j in zip(string, self.IDF)]\n","                self.tfidf_matrix.append(tfidf_list)\n","\n","            return self.tfidf_matrix\n","        else:\n","            raise Exception('TfidfVectorizer instance is not fitted yet.')\n","\n","    def fit_transform(self, sequences):\n","        self.fit(sequences)\n","        return self.transform(sequences)"],"metadata":{"id":"hJzU44eTNwNp","executionInfo":{"status":"ok","timestamp":1644679432903,"user_tz":-540,"elapsed":9,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["model = TfidfVectorizer(Tokenizer())\n","model.fit(list_sequence)\n","model.transform(list_sequence)\n","model.fit_transform(list_sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oz-jn-6ZI5A9","executionInfo":{"status":"ok","timestamp":1644679432903,"user_tz":-540,"elapsed":9,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"6ce25822-481d-4c7b-9b1a-2ff93a9da864"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0.0, 0.6931471805599453, 0.6931471805599453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n"," [0.0, 0.0, 0.0, 0.6931471805599453, 0.6931471805599453, 0.0, 0.0, 0.0, 0.0],\n"," [0.0, 0.0, 0.0, 0.0, 0.0, 0.6931471805599453, 0.6931471805599453, 0.0, 0.0],\n"," [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6931471805599453, 0.6931471805599453]]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":[""],"metadata":{"id":"byNOzI0_db51","executionInfo":{"status":"ok","timestamp":1644679432904,"user_tz":-540,"elapsed":7,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":8,"outputs":[]}]}